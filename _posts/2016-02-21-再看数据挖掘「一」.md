--
layout: post
title: 再看数据挖掘（一）
subtitle: 本系列属原创 转载请注明原著
date: 2016-02-21
author:     "Norris"
categories: blog
tags: [数据挖掘]
---

# 1.概述

数据挖掘已然是当下做数据的人不能不知道的知识了，那么数据挖掘到底是什么呢。个人理解就是在数据中发现有价值的信息。数据挖掘的任务大概可以分为预测和描述两大类。预测包括我们常说的回归与分类，前者针对于目标变量是连续的情况，而后者针对目标变量是离散的情况。描述是做什么呢，就是概括数据中潜在的联系模式，比如相关性、趋势、聚类、异常等等。常用的技术包括分类、聚类、关联分析、异常检测等。

# 2.数据

说数据挖掘的技术之前，先得说说数据。巧妇难为无米之炊，数据对于数据分析人员的重要性不言而喻。罗老师讲过，数据分析就像做菜一样，首先得有材料（数据），然后还得洗菜（数据清洗、探索），然后得有菜谱（统计、机器学习模型），然后选择对的菜谱做菜（构建模型），最后摆摆盘（数据可视化，生成分析报告），再就是洗盘子了（规整资料）。作为基础，对于数据质量、类型的了解，对其处理的方式以及可用的模型是一个数据分析员具备的基本知识。

首先就是我们统计一开始就会学到的数据的类型：定类、定序、定距、定比。不多说了。而对于每种类型数据常用的方法可以归纳为：

类型|方法|变换方法
----|----|----
定类|众数，熵，列联相关|一对一变换
定序| 中位数、百分位数、秩相关、游程检验、符号检验|单调变换
定距|均值、方差、皮尔逊相关系数、t和F检验|正彷射变换
定比|几何平均数（环比）、调和平均数（相对变化率）、百分比变差|scalar

同时数据按照不同的分类标准也可以分为：连续数据、离散数据。对称数据、非对称数据（文档词频矩阵）。

一般在观察一个数据集的时候考虑三个方面的特性：维度（变量个数），稀疏性，分辨率。通过这三个方面来了解这个数据集的基本特性。

下面说说数据质量的问题，数据的质量关乎整个数据分析结构的优劣，数据的质量主要由这几个方面构成：

1. 测量误差和数据收集错误 
	
	测量误差在一定程度上是无法避免的，有可能是系统的也有可能是随机的。收集错误，一般都可以很好的纠正。

2. 噪声和伪像

	噪声是测量误差的随机部分，无法避免。一个算法的鲁棒性就是来衡量这个算法是否能应对噪声的干扰。数据错误或者确定性的失真就是伪像。

3. 精度、偏倚、准确率

4. 离群点 
	
	也称为异常值，离群点与噪声不同，是人们感兴趣的对象，如：信用卡欺诈。

5. 缺失值

	缺失值常用的处理方法： 删除数据或者属性；估计缺失值（众数、中位数、有分布的随机抽样）；忽略缺失值。

为了让数据更适合模型，为了改善模型、减少时间、降低成本、提高质量，常常需要对数据进行预处理。

1. 根据需要处理的数据规模，对碎片化的数据进行规整。例如我只需要年销售额的数据，而只有销售额的数据，就需要事先进行聚集。

2. 抽样。 选取具有代表性的样本。 常用的抽样方法有：简单随机抽样、分层抽样、整群抽样、有序抽样等等。

	确定正确的样本容量是一个麻烦事。可以采用渐进抽样的方法（可以理解为逐步试验），逐步增加样本容量，观察结果的准确性，当当本容量的增加使得结果准确定的增幅小于一个可以接受的阈值时，即可以停止，确定样本容量了。

3. 降维。Curse of Dimensionality听着名字就觉得太可怕了。降维也是近代统计研究的一个重要方向。在数据预处理的阶段我们就可以开始着手处理这个问题了。降低了维度首先会提高算法的性能，在这更容易理解（人对抽象空间的理解能力有限），而且可以方便与可视化。最最常用的线性技术就是大名鼎鼎的PCA了，还有他的好兄弟SVD。针对时间序列的数据，莫过于傅里叶变换了，以及小波变换。

	还有一种方法就要对数据的含义有很好的理解了，这就需要这个领域的专家才可以。比如人为的特征提取，处理一个问题，专家觉得这个问题的主要影响方面有哪几个就把哪几个变量作为输入。还有新特征的构造，也需要专家的指点，比如区分一堆金属，我们有他们的质量与体积，这两个变量不好利用，我们就创建一个新的变量：密度，这就是特征的构造。

4. 离散化。对于很多分类的模型，他们的目标变量往往是连续的，如何正确的将连续变量映射到离散空间也是有学问的。如果是非监督的方法，我们常用的就是分个区间，把对应的数标记为这个区间的类型，区间的划分主要有：等宽方法，等频率方法，以及K-means等聚类方法。监督的方法就可以使用熵来定义，先分成两部分，是的熵最小，取较大熵区间再分，循环往复。

	不仅仅连续变量要离散化，离散的变量有时候还需要二元化。（为什么是二元化呢？因为常见的模型都是这样简化的。比如决策树）这时候就能体现出进制的伟大了！（6个人60桶酒问题）不过这种情况下常常会出现共线性的问题（类似dummy variable的情况）。或者就直接使用dummy variable。

5. 根据自己经验的总结，感觉如果不是非常在乎度量的大小的话，尽量先标准化数据。（此条结果有待验证，有错误希望大家指出）

接下来再说说相似性、相异性的度量，为什么把这个也放在预处理这个阶段呢？因为许多算法例如聚类，KNN，异常检测等都需要相似相异的概念来衡量数据之间的亲疏远近，例如做系统聚类时，当计算出距离矩阵时就不需要原始数据了。如果不需要复杂的度量，可以参考下表：

属性类型|相异度|相似度
----|----|----
定类数据|dummy variable(相同取1，不同取0）|类似示性函数
定序|$$d=\frac{\lvert x-y \rvert}{n-1}$$|$$s=1-d$$
定距或定比| $$d=\lvert x-y \rvert $$|$$s=-d,s=\frac{1}{1+d},s=e^{-d},s=1-\frac{d-min\_d}{max\_d-min\_d}$$

面对复杂的相异度我们经常使用距离这个概念，常用的距离有欧几里得距离，其实是明可夫斯基距离的一种特殊形式，也是L2范数。还有常用的就是L1范数，上确界距离。还有包括马氏距离也比较常用。

复杂的相似度度量。如果数据是离散的可以使用混淆矩阵。基于混淆矩阵的概念有简单匹配系数（SMC），Jaccard系数（面对非对称的数据），以及余弦相似度（对象规范化，减少计算时间，非对称数据常用，量值重要时不要用）。面对连续的数据，可以使用常用的皮尔逊相关系数，其中可以选中中位数、绝对差、trim均值作为改进方法。

距离的选择还可以使用加权的方法，有点类似核函数的意思。

# 3.探索数据

数据清洗好了，先看看数据大概是个什么样子吧。这个阶段常用的技术就是汇总（计算均值方差），可视化（画个矩阵散点图看看相关性）还有不怎么明白的OLAP。

数据汇总学过统计概率论的人都十分明白啦，就说受可视化吧。

低位数据的可视化就是统计中常用的一些方法，非常好用。包括，茎叶图、直方图、箱线图、饼图、散点图等。

高维数据的可视化就不能仅仅使用坐标这个概念了，可以加入颜色、大小、形状等等来展示不同的维度。还有非常嘻哈的Chernoff脸！

OLAP不怎么懂，不多说了，大家可以自行查找文献。

# 4.决策树

OK，菜都洗好了，就准备一下做什么菜吧。菜谱可是很重要的。从这开始就来讲讲各种常见的算法吧。先从决策树开始讲起。

决策树是在做什么呢，就是分类，分类又可以干什么呢？看了第一节的话，大家应该知道就是预测和描述两个功能。

解决分类问题的一般方法就是给定一个训练集，来训练一个分类模型，然后再用检验集来评估其性能，可以用混淆矩阵的概念来做。

决策树的概念非常形象，就像一颗倒置的树，从根节点出发，到达不同的内部节点，最后终结在叶节点。

如何构建一个决策树呢？大致的思想可以一句话说明，选取一个属性测试条件，将记录划分成越来越小，越来越纯的子集，使得最后的子集中的所有记录属于同一个类。当然这是最理想的情况，有可能无法选出一个属性测试条件，也有可能不能继续划分子集。

解决决策构建的两个重要问题就是如何分裂训练记录，以及如何停止分裂过程。

首先看如何分裂训练记录，这就要先知道都什么什么样的“分裂”。有的属性只有两个可能的结果，有的属性可能有多个，有的属性是序数属性，其结果不能随意分割，连续属性的分裂方法那就太多了。

那么如何选择最佳的分裂方式呢？

要知道分裂就是为了让子集更纯，如何定义纯呢？我们用熵来定义混乱即可，此外还可以供Gini指数、错误率、信息增益来作为混乱度的评价。但是这样定义的不纯度的度量更有利于具有大量不同值得属性，所以可以采用增益率来作为划分标准。增益率如下定义：

$$ Gain ratio = \frac{\varDelta_{info}}{Split\  Info} $$

其中$$\varDelta_{info}$$为信息增益，$$Split\  Info=-\Sigma^k_{i=1}P(v_i)log_2P(V_i)$$，k是划分的总数。

决策树划分好了，但是很有可能出现了过度拟合的情况。什么情况会导致过分拟合呢？首先是噪声，第二是样本量过小，第三是维度过多（多重选择问题，50个人预测至少预测对10只股票中的8只得概率是0.9399）。

如何处理过度拟合的问题呢？两种方法，先剪枝，当不纯度量增量小于某个阈值的时候就停止分裂。缺点是，阈值很难选，而且即使当前的增益率很低，接下来的字数的增益率有可能很高，剪掉了较好的子树。还有一种方法是后剪枝，后剪枝也有两种常用的方法，用新的叶节点替换子树（子树替换），或用子树中最常见的分支代替子树（子树提升）。

那么一个决策树出现了，如何评价他的性能呢？

1. Holdout 简单来说就是我们最常用的划分训练集和检验集的方法。局限性很多：
	
	a. 用于训练样本的数据变小。

	b. 模型可能高度依赖于训练集与检验集的构成。

	c. 训练集与检验集之间不独立。

2. 随机二次抽样 多次重复的Holdout方法，将准确率取均值即可。

3. Cross-Validation交叉验证。非常常用的方法，将训练集等分成几分，选取其中的一份作为检验集剩下的作为训练集，重复以上步骤知道所有集合都被选作过检验集，然后吧结果平均即可。

4. Boostiong。神一样的Boosting。训练记录采用又放回抽样，没有抽中的记录就成为检验集的一部分。每次抽样计算一次准确率。如何计算模型的准确率呢？这里就不是平均了，是.632 bootstrap（0.632是样本量足够大时，大小为N的自助样本包含原始数据中约63.2%的数据）。它通过组合每个自助样本的准确率和由包含所有标记样本的训练集计算的准确率来构成。

$$ acc_{boot}=\frac{1}{b}\Sigma^b_{i=1}(0.632 \times \epsilon_i + 0.368 \times acc_s)$$

如何比较两个分类器呢？

记住假设检验的威力。两个模型是否有显著差别得看假设检验。决策树准确率是一个二项分布，样本量足够大的时候可以近似为正太，所以假设检验的置信区间可以正态分布来做。比较不同分类方法也是如此。

说了这么多，我们来看看决策树有什么特点吧。

1. 决策树不需要对数据进行任何假设，因为它是一种非参数的方法。

2. 决策树非常简单，易于构建，解释性也比较强。

3. 决策树有很好的鲁棒性。

4. 决策树的决策边界全部是平行于坐标轴的，对于某些数据（例如用x+y=1完全分割的数据）的分类能力很低（但是可以利用斜决策树解决，即其测试条件变为一条斜线例如：x+y=1，但是如果是这样还不如直接用线性回归）

5. 不纯度的度量的选择对决策树的影响很小，剪枝对决策树的影响可能会更大。所以如果效果不好、不满意一定要剪枝呀。

6. 找到最佳决策树是NP完全问题。