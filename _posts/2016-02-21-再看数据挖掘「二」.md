---
layout: post
title: 再看数据挖掘（二）
date: 2016-02-21
categories: blog
tags: [数据挖掘]
description: 
---

# 5.分类的其他技术

上一篇讲到了决策树的构建、修剪、性能评价等方面的知识，然而分类方法不仅仅只有决策树，还有许多种方法，这一节主要讲讲其他几个常见的方法。

## 5.1基于规则的分类器

什么是基于规则的分类器呢？简单来说就是一个if...then...的过程，符合一定的规则就将其分到符合这个规则的类中。规则的集合就称作规则集，规则集中的每一条小规则的前提成为规则前件，规则前件有许多个合取项构成，规则的预测类成为规则后件。

分类规则的优劣由两个条件左右，一个是覆盖率，一个是准确率。覆盖率是数据集中出发该规则记录所占的比例，准确率是出发该规则记录中类标号正确的比例。

如何构建一个规则分类器呢？规则集的产生需要满足两个条件：互斥规则和穷举规则。互斥即同一个记录不能同时被两个或以上的规则出发，穷举即属性集的任一组合都被规则集中的规则覆盖。如果这两个规则同时满足，就保证了每一条记录只能被有且只有一个规则触发。

然而现实中这种情况有可能是无法实现的，如果穷尽原则没有满足，我们需要建立一个前件为空的规则作为默认规则来覆盖，这个默认规则指向一个默认类。如果规则不是互斥的，规则的预测可能会相互冲突，有一下两种方法来解决：有序规则和无序规则。有序规则，将规则进行排序，若同时满足一个，则选取排序最高的规则触发。无序规则将同一记录的触发的规则指向的类进行统计，然后采用投票最多的类。无序规则的优势在于其对于噪声或者错误的鲁棒性很强，而有序的非常敏感。而且其建立的模型开销很小，不需要去维护规则的顺序。但是它的计算量很大，每一条记录都要同每一条规则进行匹配。

有序规则的构建有两种常见的方案：基于规则的排序方案和基于类的排序方案。基于规则的排序方案假设该方案之前的规则全部是不成立的，当规则数量很大的时候，在尾部的规则将很难解释。基于类的排序方案是把同一类的规则在规则集中同时出现，同一个类之前规则的相对顺序不重要，主要其中一个被触发就赋予该规则的分类。

说了这么多，那么规则到底是如何提取出来的呢？这里只说明基于类的规则排序方法的提取方法。

首先是直接方法，这里只介绍一下广泛使用的RIPPER算法。对于多类问题，先按类的频率进行排序，假设$$(y_1,y_2,...,y_c)$$是排序后的类，其中$$y_1$$是最不频繁的类，在第一次迭代中，将属于$$y_1$$的样例标记为正例，而把其他类的样例标记为反例，使用顺序覆盖算法产生区分正例和反例的规则（顺序覆盖算法即先找出能够覆盖最多样例的规则，然后将这个规则覆盖的样例删掉，选取剩下集合中覆盖样例最多的规则，如此循环）。接下来提取区分$$y_2$$和其他类的规则。重复该过程，直到剩下类$$y_c$$，此时将$$y_c$$作为默认类。

直接方法过后一定是一个间接方法。间接方法由决策树等分类器构建，然后将其中一些规则简化即可。

那么大家肯定要问了，既然如此不如直接用决策树好了。基于规则的分类器有什么好处呢？就是更容易解释模型。基于规则的分类器实际上相比决策树更加负责，因为他可以同时出发多个条件，可以构造更加复杂的决策边界。基于规则的分类器，尤其是采用基于类的规则定序方法非常适合处理不平衡的数据集。

## 5.2最近邻分类器

大名鼎鼎的KNN，在ESL的第二章就已经开始介绍了，是一种简便而又实用的算法。将KNN之前不妨看看这句谚语。

> If it looks like a duck, quacks like a duck and walks like a duck, it‘s a duck.

KNN就是这么一种思想，如果某个目标值周围的点都是属于某一个类，那么这个点也是属于这个类的。

KNN算法的关键在于邻近点个数$$k$$的选取，太小了容易受噪声影响，过度拟合，过大的话可能会导致错误分类，而且最邻近也变得不那么“邻近”了。

不多说KNN了，大家有兴趣可以去看[ESL第二章](http://norris-niu.github.io/blog/2016/02/02/ESL-第二章/)的讲解。

## 5.3贝叶斯分类器

说到Bayes，这个都不需要用大名鼎鼎来介绍了，统计有一个分支就是贝叶斯统计。贝叶斯公式想必大家都值了，它为什么这么牛呢，**因为它把先验知识和从数据中提取出的信息相结合起来了！**大家的先验知识就是我们所说的先验概率，而结合了我们数据中的信息所得到的概率（一般是个条件概率）就是后验概率啦。

那么我们接下来就介绍两种分类器。

### 5.3.1朴素贝叶斯分类器

英文叫做Naive Bayes，钟老师说过这个Naive就是说这个分类器太天真了，不过确实也很好用（没有对的模型，只有有用的模型）。为什么说它天真呢，因为他的假设条件一看就不可能成立，它假设每个属性（每个变量）之间条件独立。什么是条件独立？

$$ P(X|Y,Z)=P(X|Z)$$

以上公式成立即说明X,Z条件独立于Y，上面的公式也可以写成：

$$P(X|Y,Z)=P(X|Z) \times P(Y|Z)$$

朴素贝叶斯的假设条件就可以表示为如下：

$$P(X|Y=y)=\prod\limits_{i=1}^dP(X_i|Y=y)$$

其中$$X=\left\{ X_1,X_2,...,X_d\right\}$$包含d个属性。

那么每个后验概率就是：

$$P(Y|X)=\frac{P(Y)\prod_{i=1}^dP(X_i|Y)}{P(X)}$$

[ESL第二章](http://norris-niu.github.io/blog/2016/02/02/ESL-第二章/)讲到贝叶斯方法通过最小化Loss Function之后，得到结果就是选取某个最大化后验概率条件的类。

所以我们的目标就是最大化上面这个后验概率。如何估计这个后验概率呢？如果目标是离散的情况，可以直接用频率来代替。那么如果是连续变量的话，第一种方法就是预处理中提到的将连续变量离散化，第二种方法就是假定变量服从某个分布，例如正太，然后用样本均值和方差替代正态分布中的参数，再计算出每个样本点对应的概率。如果大家基础比较好，可能会说连续变量的概率密度中某一点的概率为0啊，其实这里是用了一个近似，假设在目标点周围一个足够小的区间内计算概率，近似的用目标点的值带入即可（page 143）。

如果大家实际计算过贝叶斯的问题会发现，这个连乘的概率非常脆弱，若果某一项为0，不管其他项多大，结果都是0，而某个样本出现频率为0不一定代表他真的没有出现，只是没有被观测到。所以如何解决这个问题呢？一种方法是m估计，强行的给他们一个小概率。还有一个方法是Good-Turing Estimate（古德-图灵估计，可以参考「数学之美」page 35），看到过，不知道是否可行，有待验证。

评价Bayes分类器的一个标准就是Bayes error rate，即贝叶斯误差率，是被错误分到别的类下面的样本点的比率（离散情况很好理解，那么连续的呢？）。

朴素Bayes分类器有如下特点：

1. 面对孤立的噪声点和无关属性，Bayes Classifier is robust，噪声点的影响会被平均，而无关变量基本是一个均匀分布，不会影响后验概率的比较。

2. 相关属性会降低Bayes分类器的性能，因为不符合假设呀！

### 5.3.2贝叶斯信念网络（Bayesian belief networks,BBN）

为什么不用Naive了呢？就是因为它太Naive了，天真的让人不敢相信。所以就出现了贝叶斯信念网络，他不要求所有的属性条件都独立，而是允许指定哪些属性条件独立。

贝叶斯信念网络有两个主要的组成部分：一个有向无环图（表示各变量之间的依赖性），一个概率表（将各个结点与其父结点关联起来）。

贝叶斯网络一个重要性质是：贝叶斯网络中的一个结点，如果它的父结点已知，则它条件独立于它的所有非后代结点。

概率表的算法是，如果该点没有父母结点，则表中只包含其先验概率。若含有父结点，则该表包含条件概率
$$P(X\|Y_1,Y_2,...,Y_k)$$

建立一个BNN首先需要用“专家”知识对所有的属性进行排序，然后从头开始，每个属性将其前面的所有属性作为条件，不剔除无关的条件，根据最后的条件概率画出有向图。

可以看出BNN的先决条件是属性的排序，不同的排序将会影响结果，所以这也是BNN的缺陷之一，如何解决呢？就是尽量将属性分为原因与结果，这样就不会有太多的排序可能出现，这就需要“专家”的力量啦！